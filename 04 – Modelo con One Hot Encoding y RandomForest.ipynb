{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 04 - MODELO ALTERNATIVO: Random Forest con Pipeline (One-Hot)\n",
        "\n",
        "## Entrega Final - Proyecto Modelos y Simulación de Sistemas\n",
        "\n",
        "### Integrantes del equipo\n",
        "| Nombre completo | Programa |\n",
        "| :--- | :--- |\n",
        "| Cristian David Diez Lopez | Ingeniería de Sistemas |\n",
        "| Rafael Ángel Alemán Castillo | Ingeniería de Sistemas |\n",
        "| Jonatan Romero Arrieta | Ingeniería de Sistemas |\n",
        "\n",
        "---\n",
        "\n",
        "### Descripción del enfoque (Aproximación Distinta 2)\n",
        "\n",
        "Este notebook implementa una estrategia **completamente distinta** al basarse en la arquitectura clásica de Scikit-Learn.\n",
        "* **Preprocesamiento (Pipeline Completo):** En lugar de usar el manejo nativo de categorías, aplicamos **One-Hot Encoding** explícito y escalado de variables numéricas (**StandardScaler**).\n",
        "* **Modelo:** Se utiliza un **RandomForestClassifier**, un modelo de ensamble (Bagging) que contrasta con el Boosting (CatBoost) de la solución principal."
      ],
      "metadata": {
        "id": "h2ayPbAZj6h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall pandas==2.2.2\n"
      ],
      "metadata": {
        "id": "pYB_MWunLDaO",
        "outputId": "2cf33530-57fa-4c1c-84cf-5cc638ce9f20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==2.2.2\n",
            "  Using cached pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting numpy>=1.26.0 (from pandas==2.2.2)\n",
            "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas==2.2.2)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas==2.2.2)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas==2.2.2)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.2)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.5 pandas-2.2.2 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "numpy",
                  "pytz",
                  "six"
                ]
              },
              "id": "d70ec40923ae4c078c13c2bcbfb76d17"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Definición del Pipeline de Preprocesamiento\n",
        "\n",
        "A diferencia de los otros notebooks, aquí construimos un **Pipeline de Scikit-Learn** para asegurar un tratamiento riguroso de los datos y evitar el \"data leakage\":\n",
        "\n",
        "1.  **Transformación Numérica (`num_transformer`):**\n",
        "    * **Imputación:** Se usa la **mediana** (`SimpleImputer`) para rellenar huecos.\n",
        "    * **Escalado:** Se aplica **StandardScaler** para normalizar los datos (media 0, desviación 1), lo cual es crucial cuando se combinan distancias o varianzas.\n",
        "\n",
        "2.  **Transformación Categórica (`cat_transformer`):**\n",
        "    * **Imputación:** Se rellenan los nulos con una constante `\"missing\"`.\n",
        "    * **Codificación:** Se aplica **OneHotEncoder** con `handle_unknown='ignore'`. Esto explota las variables categóricas en múltiples columnas binarias (0/1), una representación necesaria para modelos como Random Forest que no manejan texto nativamente."
      ],
      "metadata": {
        "id": "cce-VVDCkEKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Definición y Entrenamiento del Modelo Random Forest\n",
        "\n",
        "El modelo seleccionado es un **RandomForestClassifier** integrado directamente en el pipeline.\n",
        "* **n_estimators=100:** Creamos un bosque de 100 árboles de decisión.\n",
        "* **max_depth=10:** Limitamos la profundidad para controlar el sobreajuste, dado que el One-Hot Encoding incrementa significativamente la dimensionalidad de los datos de entrada.\n",
        "* **n_jobs=-1:** Utilizamos todos los núcleos del procesador para paralelizar el entrenamiento."
      ],
      "metadata": {
        "id": "v4CI_JS8kFOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr5NGpzWJn-s",
        "outputId": "ad862788-84f1-4834-bbb7-5b24ec9f8638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.41031201313739524\n",
            "Archivo submission_04.csv generado!\n"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# 04 - MODELO ONE-HOT + RANDOM FOREST\n",
        "# ===============================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Función de limpieza\n",
        "# ---------------------------\n",
        "def limpiar_para_onehot(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Quitar ID\n",
        "    if \"ID\" in df.columns:\n",
        "        df.drop(columns=[\"ID\"], inplace=True)\n",
        "\n",
        "    # Mapear target\n",
        "    if \"RENDIMIENTO_GLOBAL\" in df.columns:\n",
        "        df[\"RENDIMIENTO_GLOBAL\"] = (\n",
        "            df[\"RENDIMIENTO_GLOBAL\"]\n",
        "            .astype(str)\n",
        "            .str.strip()\n",
        "            .str.lower()\n",
        "        )\n",
        "\n",
        "        mapa = {\n",
        "            \"bajo\": 0,\n",
        "            \"medio-bajo\": 1,\n",
        "            \"medio bajo\": 1,\n",
        "            \"medio-alto\": 2,\n",
        "            \"medio alto\": 2,\n",
        "            \"alto\": 3\n",
        "        }\n",
        "\n",
        "        df[\"RENDIMIENTO_GLOBAL\"] = df[\"RENDIMIENTO_GLOBAL\"].map(mapa)\n",
        "\n",
        "        df[\"RENDIMIENTO_GLOBAL\"] = df[\"RENDIMIENTO_GLOBAL\"].fillna(\n",
        "            df[\"RENDIMIENTO_GLOBAL\"].mode()[0]\n",
        "        )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Cargar y limpiar datos\n",
        "# ---------------------------\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "train = limpiar_para_onehot(train)\n",
        "\n",
        "X = train.drop(columns=[\"RENDIMIENTO_GLOBAL\"])\n",
        "y = train[\"RENDIMIENTO_GLOBAL\"]\n",
        "\n",
        "cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
        "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Preprocesador: Imputación + OneHot\n",
        "# ---------------------------\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "        ]), cat_cols),\n",
        "\n",
        "        (\"num\", Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
        "        ]), num_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Pipeline con modelo\n",
        "# ---------------------------\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"model\", RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=25,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Split\n",
        "# ---------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Entrenar\n",
        "# ---------------------------\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Validar\n",
        "pred = pipeline.predict(X_val)\n",
        "print(\"Accuracy:\", accuracy_score(y_val, pred))\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Predicción final\n",
        "# ---------------------------\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "ids = test[\"ID\"].copy()\n",
        "test = limpiar_para_onehot(test)\n",
        "\n",
        "test_pred = pipeline.predict(test)\n",
        "\n",
        "reverse_map = {0: \"bajo\", 1: \"medio-bajo\", 2: \"medio-alto\", 3: \"alto\"}\n",
        "test_pred = pd.Series(test_pred).map(reverse_map)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": ids,\n",
        "    \"RENDIMIENTO_GLOBAL\": test_pred\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission_04.csv\", index=False)\n",
        "print(\"Archivo submission_04.csv generado!\")\n",
        "\n"
      ]
    }
  ]
}
